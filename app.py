{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMbc4uG6JjdK8Y06D3v2kd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import streamlit as st\n","import pickle\n","import re\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","#loading models\n","clf = pickle.load(open('models/clf.pkl','rb'))\n","tfidfd = pickle.load(open('models/tfidf.pkl','rb'))\n","\n","def clean_resume(resume_text):\n","    clean_text = re.sub('http\\S+\\s*', ' ', resume_text)\n","    clean_text = re.sub('RT|cc', ' ', clean_text)\n","    clean_text = re.sub('#\\S+', '', clean_text)\n","    clean_text = re.sub('@\\S+', '  ', clean_text)\n","    clean_text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', clean_text)\n","    clean_text = re.sub(r'[^\\x00-\\x7f]', r' ', clean_text)\n","    clean_text = re.sub('\\s+', ' ', clean_text)\n","    return clean_text\n","# web app\n","def main():\n","    st.title(\"Resume Screening App\")\n","    uploaded_file = st.file_uploader('Upload Resume', type=['txt','pdf'])\n","\n","    if uploaded_file is not None:\n","        try:\n","            resume_bytes = uploaded_file.read()\n","            resume_text = resume_bytes.decode('utf-8')\n","        except UnicodeDecodeError:\n","            # If UTF-8 decoding fails, try decoding with 'latin-1'\n","            resume_text = resume_bytes.decode('latin-1')\n","\n","        cleaned_resume = clean_resume(resume_text)\n","        input_features = tfidfd.transform([cleaned_resume])\n","        prediction_id = clf.predict(input_features)[0]\n","        st.write(prediction_id)\n","\n","        # Map category ID to category name\n","        category_mapping = {\n","            15: \"Java Developer\",\n","            23: \"Testing\",\n","            8: \"DevOps Engineer\",\n","            20: \"Python Developer\",\n","            24: \"Web Designing\",\n","            12: \"HR\",\n","            13: \"Hadoop\",\n","            3: \"Blockchain\",\n","            10: \"ETL Developer\",\n","            18: \"Operations Manager\",\n","            6: \"Data Science\",\n","            22: \"Sales\",\n","            16: \"Mechanical Engineer\",\n","            1: \"Arts\",\n","            7: \"Database\",\n","            11: \"Electrical Engineering\",\n","            14: \"Health and fitness\",\n","            19: \"PMO\",\n","            4: \"Business Analyst\",\n","            9: \"DotNet Developer\",\n","            2: \"Automation Testing\",\n","            17: \"Network Security Engineer\",\n","            21: \"SAP Developer\",\n","            5: \"Civil Engineer\",\n","            0: \"Advocate\",\n","        }\n","\n","        category_name = category_mapping.get(prediction_id, \"Unknown\")\n","\n","        st.write(\"Predicted Category:\", category_name)\n","\n","\n","\n","# python main\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"BAGUWrmAPrSB","executionInfo":{"status":"error","timestamp":1719183029430,"user_tz":-330,"elapsed":19,"user":{"displayName":"armaan khan","userId":"04961420720707932216"}},"outputId":"10c2453b-dc80-45aa-fd58-65b5b523156e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'models/clf.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-7bd2462a4ac2>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#loading models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/clf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtfidfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/tfidf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/clf.pkl'"]}]},{"source":["import streamlit as st\n","import pickle\n","import re\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Loading models - Make sure these files exist in a 'models' directory\n","# relative to where you run this script\n","clf = pickle.load(open('models/clf.pkl', 'rb'))\n","tfidfd = pickle.load(open('models/tfidf.pkl', 'rb'))\n","\n","\n","def clean_resume(resume_text):\n","    clean_text = re.sub('http\\S+\\s*', ' ', resume_text)\n","    clean_text = re.sub('RT|cc', ' ', clean_text)\n","    clean_text = re.sub('#\\S+', '', clean_text)\n","    clean_text = re.sub('@\\S+', '  ', clean_text)\n","    clean_text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', clean_text)\n","    clean_text = re.sub(r'[^\\x00-\\x7f]', r' ', clean_text)\n","    clean_text = re.sub('\\s+', ' ', clean_text)\n","    return clean_text\n","\n","\n","# Web app\n","def main():\n","    st.title(\"Resume Screening App\")\n","    uploaded_file = st.file_uploader('Upload Resume', type=['txt', 'pdf'])\n","\n","    if uploaded_file is not None:\n","        try:\n","            resume_bytes = uploaded_file.read()\n","            resume_text = resume_bytes.decode('utf-8')\n","        except UnicodeDecodeError:\n","            # If UTF-8 decoding fails, try decoding with 'latin-1'\n","            resume_text = resume_bytes.decode('latin-1')\n","\n","        cleaned_resume = clean_resume(resume_text)\n","        input_features = tfidfd.transform([cleaned_resume])\n","        prediction_id = clf.predict(input_features)[0]\n","\n","        # Map category ID to category name\n","        category_mapping = {\n","            15: \"Java Developer\",\n","            23: \"Testing\",\n","            8: \"DevOps Engineer\",\n","            20: \"Python Developer\",\n","            24: \"Web Designing\",\n","            12: \"HR\",\n","            13: \"Hadoop\",\n","            3: \"Blockchain\",\n","            10: \"ETL Developer\",\n","            18: \"Operations Manager\",\n","            6: \"Data Science\",\n","            22: \"Sales\",\n","            16: \"Mechanical Engineer\",\n","            1: \"Arts\",\n","            7: \"Database\",\n","            11: \"Electrical Engineering\",\n","            14: \"Health and fitness\",\n","            19: \"PMO\",\n","            4: \"Business Analyst\",\n","            9: \"DotNet Developer\",\n","            2: \"Automation Testing\",\n","            17: \"Network Security Engineer\",\n","            21: \"SAP Developer\",\n","            5: \"Civil Engineer\",\n","            0: \"Advocate\",\n","        }\n","\n","        category_name = category_mapping.get(prediction_id, \"Unknown\")\n","\n","        st.write(\"Predicted Category:\", category_name)\n","\n","\n","# python main\n","if __name__ == \"__main__\":\n","    main()"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"0LtbazIWRT2p","executionInfo":{"status":"error","timestamp":1719182663367,"user_tz":-330,"elapsed":448,"user":{"displayName":"armaan khan","userId":"04961420720707932216"}},"outputId":"9bb7a869-57ca-4b9f-e715-f89e58f8c2fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'models/clf.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5ee238c31590>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Loading models - Make sure these files exist in a 'models' directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# relative to where you run this script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/clf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtfidfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/tfidf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/clf.pkl'"]}]},{"source":["import streamlit as st\n","import pickle\n","import re\n","import nltk\n","import os\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Check if the 'models' directory exists, and create it if not\n","if not os.path.exists('models'):\n","    os.makedirs('models')\n","    print(\"Created 'models' directory.\")\n","\n","# Loading models - Make sure these files exist in a 'models' directory\n","# relative to where you run this script\n","# If the files don't exist, you'll need to train and save your models first\n","if os.path.exists('models/clf.pkl') and os.path.exists('models/tfidf.pkl'):\n","    clf = pickle.load(open('models/clf.pkl', 'rb'))\n","    tfidfd = pickle.load(open('models/tfidf.pkl', 'rb'))\n","else:\n","    st.error(\"Error: Model files not found. Please make sure 'clf.pkl' and 'tfidf.pkl' exist in the 'models' directory.\")\n","    st.stop()  # Stop execution if models are not found\n","\n","def clean_resume(resume_text):\n","    clean_text = re.sub('http\\S+\\s*', ' ', resume_text)\n","    clean_text = re.sub('RT|cc', ' ', clean_text)\n","    clean_text = re.sub('#\\S+', '', clean_text)\n","    clean_text = re.sub('@\\S+', '  ', clean_text)\n","    clean_text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', clean_text)\n","    clean_text = re.sub(r'[^\\x00-\\x7f]', r' ', clean_text)\n","    clean_text = re.sub('\\s+', ' ', clean_text)\n","    return clean_text\n","\n","\n","# Web app\n","def main():\n","    st.title(\"Resume Screening App\")\n","    uploaded_file = st.file_uploader('Upload Resume', type=['txt', 'pdf'])\n","\n","    if uploaded_file is not None:\n","        try:\n","            resume_bytes = uploaded_file.read()\n","            resume_text = resume_bytes.decode('utf-8')\n","        except UnicodeDecodeError:\n","            # If UTF-8 decoding fails, try decoding with 'latin-1'\n","            resume_text = resume_bytes.decode('latin-1')\n","\n","        cleaned_resume = clean_resume(resume_text)\n","        input_features = tfidfd.transform([cleaned_resume])\n","        prediction_id = clf.predict(input_features)[0]\n","\n","        # Map category ID to category name\n","        category_mapping = {\n","            15: \"Java Developer\",\n","            23: \"Testing\",\n","            8: \"DevOps Engineer\",\n","            20: \"Python Developer\",\n","            24: \"Web Designing\",\n","            12: \"HR\",\n","            13: \"Hadoop\",\n","            3: \"Blockchain\",\n","            10: \"ETL Developer\",\n","            18: \"Operations Manager\",\n","            6: \"Data Science\",\n","            22: \"Sales\",\n","            16: \"Mechanical Engineer\",\n","            1: \"Arts\",\n","            7: \"Database\",\n","            11: \"Electrical Engineering\",\n","            14: \"Health and fitness\",\n","            19: \"PMO\",\n","            4: \"Business Analyst\",\n","            9: \"DotNet Developer\",\n","            2: \"Automation Testing\",\n","            17: \"Network Security Engineer\",\n","            21: \"SAP Developer\",\n","            5: \"Civil Engineer\",\n","            0: \"Advocate\",\n","        }\n","\n","        category_name = category_mapping.get(prediction_id, \"Unknown\")\n","\n","        st.write(\"Predicted Category:\", category_name)\n","\n","\n","# python main\n","if __name__ == \"__main__\":\n","    main()"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDeR-A0fRjK4","executionInfo":{"status":"ok","timestamp":1719183040006,"user_tz":-330,"elapsed":435,"user":{"displayName":"armaan khan","userId":"04961420720707932216"}},"outputId":"b899d0bf-bf51-4e49-e43f-4bd27894b660"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]}]}